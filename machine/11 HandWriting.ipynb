{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "from tensorflow.compat.v1 import set_random_seed\n",
    "set_random_seed(404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_path='handwriting/digit_xtrain.csv'\n",
    "y_train_path='handwriting/digit_ytrain.csv'\n",
    "x_test_path='handwriting/digit_xtest.csv'\n",
    "y_test_path='handwriting/digit_ytest.csv'\n",
    "nr_classes=10\n",
    "valid_size=10000\n",
    "total_inputs=28*28*1 # image height * width * channels\n",
    "\n",
    "logging_path='tb_mnist_logs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 20.9 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "x_train_all=np.loadtxt(x_train_path, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 3.87 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "x_test=np.loadtxt(x_test_path, delimiter=',', dtype=int)\n",
    "y_train_all=np.loadtxt(y_train_path, delimiter=',', dtype=int)\n",
    "y_test=np.loadtxt(y_test_path, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescale the data to from 255 to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_all, x_test = x_train_all/255.0 , x_test/ 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert our Y set lables to One Hot Encodeing with numpy eye()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all= np.eye(nr_classes)[y_train_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=np.eye(nr_classes)[y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(60000, 10)"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make validation sets from trainig datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val=x_train_all[:valid_size]\n",
    "y_val=y_train_all[:valid_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train_all[valid_size:]\n",
    "y_train=y_train_all[valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(50000, 784)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(10000, 784)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting and Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32, shape=[None, total_inputs],name=\"X\")\n",
    "y=tf.placeholder(tf.float32, shape=[None, nr_classes],name=\"labels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architcture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining our Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs= 50\n",
    "learning_rate =1e-3\n",
    "n_hidden1=512\n",
    "n_hidden2=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('Hidden_1'):\n",
    "#     init_w1=tf.compat.v1.truncated_normal(shape=[total_inputs, n_hidden1], stddev=0.1, seed=42)\n",
    "#     w1= tf.Variable(init_w1,name='W1')\n",
    "#     init_b1=tf.constant(value=0.0, shape=[n_hidden1])\n",
    "#     b1=tf.Variable(init_b1,name='b1')\n",
    "#     layer1_in = tf.matmul(x, w1)+ b1\n",
    "#     layer1_out= tf.nn.relu(layer1_in)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('Hidden_2'):\n",
    "#     init_w2=tf.compat.v1.truncated_normal(shape=[n_hidden1, n_hidden2], stddev=0.1, seed=42)\n",
    "#     w2= tf.Variable(init_w2,name='W2')\n",
    "#     init_b2=tf.constant(value=0.0, shape=[n_hidden2])\n",
    "#     b2=tf.Variable(init_b2,name='b2')\n",
    "#     layer2_in = tf.matmul(layer1_out, w2)+ b2\n",
    "#     layer2_out= tf.nn.relu(layer2_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('Output_Layer'):\n",
    "#     init_w3=tf.compat.v1.truncated_normal(shape=[n_hidden2, nr_classes], stddev=0.1, seed=42)\n",
    "#     w3= tf.Variable(initial_value=init_w3,name='W3')\n",
    "#     init_b3=tf.constant(value=0.0, shape=[nr_classes])\n",
    "#     b3=tf.Variable(init_b3,name='b3')\n",
    "#     layer3_in = tf.matmul(layer2_out, w3)+ b3\n",
    "#     output= tf.nn.softmax(layer3_in)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automate to create the Layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_layer(input,weight_dim, bias_dim, name):\n",
    "    with tf.name_scope(name):\n",
    "\n",
    "        init_w=tf.truncated_normal(shape=weight_dim, stddev=0.1, seed=42)\n",
    "        w= tf.Variable(initial_value=init_w,name='W')\n",
    "\n",
    "        init_b=tf.constant(value=0.0, shape=bias_dim)\n",
    "        b=tf.Variable(initial_value=init_b,name='B')\n",
    "\n",
    "        layer_in = tf.matmul(input, w)+ b\n",
    "\n",
    "        if (name=='out'):\n",
    "            layer_out = tf.nn.softmax(layer_in)\n",
    "        else:\n",
    "            layer_out = tf.nn.relu(layer_in)\n",
    "        \n",
    "        tf.summary.histogram('weights',w)\n",
    "        tf.summary.histogram('biases',b)\n",
    "\n",
    "        return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the layer with the Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_1=setup_layer(x,weight_dim=[total_inputs,n_hidden1],bias_dim=[n_hidden1],name='layer_1')\n",
    "\n",
    "# layer_2=setup_layer(layer_1,weight_dim=[n_hidden1,n_hidden2],bias_dim=[n_hidden2],name='layer_2')\n",
    "\n",
    "# output=setup_layer(layer_2,weight_dim=[n_hidden2,nr_classes],bias_dim=[nr_classes],name='out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1=setup_layer(x,weight_dim=[total_inputs,n_hidden1],bias_dim=[n_hidden1],name='layer_1')\n",
    "\n",
    "layer_drop=tf.nn.dropout(layer_1,keep_prob=0.8, name='layer_drop')\n",
    "\n",
    "layer_2=setup_layer(layer_drop,weight_dim=[n_hidden1,n_hidden2],bias_dim=[n_hidden2],name='layer_2')\n",
    "\n",
    "output=setup_layer(layer_2,weight_dim=[n_hidden2,nr_classes],bias_dim=[nr_classes],name='out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dirs created successfully\n"
    }
   ],
   "source": [
    "# Create the folder to be used with Tensorboard\n",
    "folder_name=f'{n_hidden1}-{n_hidden2}-D- at {strftime(\"%H_%M\")}'\n",
    "directory=os.path.join(logging_path,folder_name)\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as exception:\n",
    "    print(exception.strerror)\n",
    "else:\n",
    "    print(\"dirs created successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_name=f'Model 1 at {strftime(\"%H:%M\")}'\n",
    "# folder_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Loss, Optimizer, Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Loss'):\n",
    "    loss= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "    train_step=optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy_metrics'):    \n",
    "    correct_pred=tf.equal(tf.argmax(output, axis=1),tf.argmax(y,axis=1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('performance'):\n",
    "    tf.compat.v1.summary.scalar('accuracy', accuracy)\n",
    "    tf.compat.v1.summary.scalar('cost', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('showImage'):\n",
    "    x_image=tf.reshape(x, [-1, 28,28,1])\n",
    "    tf.summary.image('image_input',x_image,max_outputs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisation all the Variables\n",
    "init=tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Filewriter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "merage_summary=tf.summary.merge_all()\n",
    "train_writer=tf.summary.FileWriter(directory+'/train')\n",
    "train_writer.add_graph(sess.graph)\n",
    "\n",
    "validation_writer=tf.summary.FileWriter(directory+'/validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch=1000\n",
    "num_examples=y_train.shape[0]\n",
    "nr_iterations=int(num_examples/size_of_batch)\n",
    "index_in_epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "50"
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "nr_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Function to help batching the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    global num_examples\n",
    "    global index_in_epoch\n",
    "    # if we reach the end of the Dataset\n",
    "    start=index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    if index_in_epoch > num_examples:\n",
    "        start=0\n",
    "        index_in_epoch=batch_size\n",
    "\n",
    "    end=index_in_epoch\n",
    "\n",
    "    return data[start:end] , labels[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainig Loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 0 \t| Trainig Accuracy 0.8489999771118164\nEpoch 1 \t| Trainig Accuracy 0.859000027179718\nEpoch 2 \t| Trainig Accuracy 0.8659999966621399\nEpoch 3 \t| Trainig Accuracy 0.8690000176429749\nEpoch 4 \t| Trainig Accuracy 0.8740000128746033\nEpoch 5 \t| Trainig Accuracy 0.875\nEpoch 6 \t| Trainig Accuracy 0.972000002861023\nEpoch 7 \t| Trainig Accuracy 0.9789999723434448\nEpoch 8 \t| Trainig Accuracy 0.9779999852180481\nEpoch 9 \t| Trainig Accuracy 0.9819999933242798\nEpoch 10 \t| Trainig Accuracy 0.9829999804496765\nEpoch 11 \t| Trainig Accuracy 0.9829999804496765\nEpoch 12 \t| Trainig Accuracy 0.984000027179718\nEpoch 13 \t| Trainig Accuracy 0.9879999756813049\nEpoch 14 \t| Trainig Accuracy 0.9850000143051147\nEpoch 15 \t| Trainig Accuracy 0.9890000224113464\nEpoch 16 \t| Trainig Accuracy 0.9890000224113464\nEpoch 17 \t| Trainig Accuracy 0.9900000095367432\nEpoch 18 \t| Trainig Accuracy 0.9900000095367432\nEpoch 19 \t| Trainig Accuracy 0.9890000224113464\nEpoch 20 \t| Trainig Accuracy 0.984000027179718\nEpoch 21 \t| Trainig Accuracy 0.9919999837875366\nEpoch 22 \t| Trainig Accuracy 0.9900000095367432\nEpoch 23 \t| Trainig Accuracy 0.9919999837875366\nEpoch 24 \t| Trainig Accuracy 0.9909999966621399\nEpoch 25 \t| Trainig Accuracy 0.9909999966621399\nEpoch 26 \t| Trainig Accuracy 0.9900000095367432\nEpoch 27 \t| Trainig Accuracy 0.9929999709129333\nEpoch 28 \t| Trainig Accuracy 0.9909999966621399\nEpoch 29 \t| Trainig Accuracy 0.9929999709129333\nEpoch 30 \t| Trainig Accuracy 0.9919999837875366\nEpoch 31 \t| Trainig Accuracy 0.9929999709129333\nEpoch 32 \t| Trainig Accuracy 0.9940000176429749\nEpoch 33 \t| Trainig Accuracy 0.9929999709129333\nEpoch 34 \t| Trainig Accuracy 0.9940000176429749\nEpoch 35 \t| Trainig Accuracy 0.9909999966621399\nEpoch 36 \t| Trainig Accuracy 0.9929999709129333\nEpoch 37 \t| Trainig Accuracy 0.9940000176429749\nEpoch 38 \t| Trainig Accuracy 0.9940000176429749\nEpoch 39 \t| Trainig Accuracy 0.9929999709129333\nEpoch 40 \t| Trainig Accuracy 0.9940000176429749\nEpoch 41 \t| Trainig Accuracy 0.9940000176429749\nEpoch 42 \t| Trainig Accuracy 0.9940000176429749\nEpoch 43 \t| Trainig Accuracy 0.9940000176429749\nEpoch 44 \t| Trainig Accuracy 0.9940000176429749\nEpoch 45 \t| Trainig Accuracy 0.9929999709129333\nEpoch 46 \t| Trainig Accuracy 0.9940000176429749\nEpoch 47 \t| Trainig Accuracy 0.9940000176429749\nEpoch 48 \t| Trainig Accuracy 0.9940000176429749\nEpoch 49 \t| Trainig Accuracy 0.9940000176429749\nTraining Done!\n"
    }
   ],
   "source": [
    "for epoch in range(nr_epochs):\n",
    "    for i in range(nr_iterations):\n",
    "        batch_x, batch_y= next_batch(batch_size=size_of_batch, data=x_train, labels=y_train)\n",
    "\n",
    "        feed_dictionary= {x:batch_x, y:batch_y}\n",
    "        sess.run(train_step, feed_dict=feed_dictionary)\n",
    "\n",
    "    s,batch_accuracy=sess.run(fetches=[merage_summary,accuracy], feed_dict=feed_dictionary)\n",
    "    train_writer.add_summary(s,epoch)\n",
    "    print(f'Epoch {epoch} \\t| Trainig Accuracy {batch_accuracy}')\n",
    "    # ====================== Validation \n",
    "    summary=sess.run(fetches=merage_summary, feed_dict={x:x_val, y:y_val})\n",
    "    validation_writer.add_summary(summary,epoch)\n",
    "\n",
    "print(\"Training Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using pil to load our test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=28x28 at 0x20302049788>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAYAAAByDd+UAAABAElEQVR4nO2Vyw2DMAyG3ap3RoAVmMCBrXICbwIbMEEEmzBCmMA9VFR9EGLTqhIV3xET/vjHjxMzM/yQ8y/FDsEgRAREtEnwJC2aWWAYhqfnzjmVoChDIroLISI45+5C2kwvsRemaYK+76HrOkiS5CmGiG8ZR2EB4zgGY0VRcF3Xks8wM7PI0jRNgzFEVCW4j7b4CLH5C3jv2RjD3nvxmWiVhphbJc/zt+pdY5NgWZYAcCuYqqp0hzUWWmvZGMPWWrX9KksfJ03TNKttEmNV8HWkqe1bIDi8iQjatoUsy8TNLbnQ6rbQDObZiej22Pz3F5DMVPE+/Bb/P0sPwf0LXgGAJwNqzP5nHgAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "execution_count": 135
    }
   ],
   "source": [
    "img= Image.open('handwriting/test_img.png')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert the img  black_wight mode then invert then colors then to array then flat it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw=img.convert('L')\n",
    "img_array=np.invert(bw)\n",
    "img_array.shape\n",
    "test_img=img_array.ravel()\n",
    "test_img.shape\n",
    "prediction= sess.run(fetches=tf.argmax(output,axis=1), feed_dict={x:[test_img]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Prediction for test image is [2]\n"
    }
   ],
   "source": [
    "print(f'Prediction for test image is {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy on test set is 98.04%\n"
    }
   ],
   "source": [
    "test_accuracy = sess.run(fetches=accuracy, feed_dict={x:x_test, y:y_test})\n",
    "print(f'Accuracy on test set is {test_accuracy:0.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'my-test-model'"
     },
     "metadata": {},
     "execution_count": 139
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'my-test-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_writer.close()\n",
    "validation_writer.close()\n",
    "sess.close()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bit64d8e55dfcfb41aaa3a2c6da0c101c81",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}